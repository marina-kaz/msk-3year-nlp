{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hometask 1: sentiment analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_CzjN-StydR"
      },
      "source": [
        "## Домашнее задание # 1: Sentiment Analysis при помощи словарей и множеств\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnyC0lo5Q45D"
      },
      "source": [
        "> Сначала нам надо скачать дату\n",
        ">>соберите как минимум 60 (30 положительных  и 30 отрицательных) отзывов на похожие продукты (не надо мешать отзывы на отели с отзывами на ноутбуки) для составления \" тонального словаря\" (чем больше отзывов, тем лучше)\n",
        ">> 10 отзывов для проверки качества.   \n",
        ">2 балла в случае сбора путём парсинга, 1 - если найдете уже готовые данные или просто закопипастите без парсинга\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMBivptux1xV"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E90VUQGUSFp3"
      },
      "source": [
        "Я не очень долго думала, какой сайт хочу парсить - сердечко просто шепнуло *irecommend*.\n",
        "\n",
        "Жалею ли я - вопрос интересный. Скорее нет, чем да, потому что работа с таким *хорошо защищенным от DDoS атак* сайтом дала мне богатый опыт и позволила лишний раз поупражнять тепрение и выдержку.\n",
        "\n",
        "Алгоритм парсинга классический:\n",
        "* соскрейпить ссылки на достаточное количество языкового материала\n",
        "* последовательно этот материал собрать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYfIaMZ0TMVq"
      },
      "source": [
        "Имея душевные силы и желая избежать хардкодинг любой ценой, я решила, что не буду смотреть глазами и прописывать руками максимально возможную страницу. Не то чтобы у меня был шанс ее случайно достингнуть, но тогда я об этом еще не знала. \n",
        "\n",
        "В ретроспективе, конечно, выглядит забавно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs7WsjAs8Mfd",
        "outputId": "7af405e8-8e05-4531-e91a-61f5f75a04e8"
      },
      "source": [
        "page = 0\n",
        "max_page = 1\n",
        "\n",
        "review_hrefs = []\n",
        "\n",
        "while page != max_page:\n",
        "    url = f'https://irecommend.ru/content/ottenochnyi-balzam-tonika?page={page}'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print('ERROR code', response.status_code)\n",
        "        break\n",
        "    html = BeautifulSoup(response.text, 'html.parser')\n",
        "    if page == 0:\n",
        "        max_page = int(html.find_all('li', \n",
        "                                        {'class': 'pager-last last'})[0].text)\n",
        "    reviews = html.find_all('div', \n",
        "                            {'class': 'product-node'})[0].find_all('div', \n",
        "                                                                   {'class': \n",
        "                                                                    'reviewTitle'})\n",
        "    for review in reviews:\n",
        "        review_hrefs.append('https://irecommend.ru' + review.find('a').get('href'))\n",
        "    page += 1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR status code 521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icfw14pjj1i2",
        "outputId": "3be40a18-5b73-491d-ed2b-d9d66595667f"
      },
      "source": [
        "print(f'Анализируемые в настоящей работе отзывы посвящены \\\n",
        "товару с названием \"{html.title.string}\"')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Анализируемые в настоящей работе отзывы посвящены товару с названием \"Оттеночный бальзам для волос Тоника РоКОЛОР | Отзывы покупателей\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xmOmKKXFVl5",
        "outputId": "e10b7c58-befd-4b8e-ec49-520ae8a846db"
      },
      "source": [
        "print(f'Before getting banned we managed to collect {len(review_hrefs)} full review hrefs')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before getting banned we managed to collect 450 full review hrefs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED7x00fuSDm1"
      },
      "source": [
        "Сбор языкового материала заключается не только в сохранении текстов, но и в запоминании их оценок. \n",
        "\n",
        "Сохраняем тексты в файлы с соответствующими названиями, а оценки - в словарь с ключами-индексами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWHuk4PIS4_Z"
      },
      "source": [
        "review_ratings = {}"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBhCGgb6DUew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5a2ab8-587d-49fd-b674-1e357a6accb7"
      },
      "source": [
        "for index in range(len(review_hrefs)):\n",
        "    response = requests.get(review_hrefs[index], \n",
        "                            headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'})\n",
        "    # sleep(randint(1, 5))\n",
        "    if not response.status_code == 200:\n",
        "        print('ERROR status code', response.status_code)\n",
        "        break\n",
        "    html = BeautifulSoup(response.text, 'html.parser')\n",
        "    with open(f'text_{index}.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(html.find('div', {'class': 'views-field-teaser reviewText'}).text)\n",
        "    grade = int(html.find('meta', {'itemprop': 'ratingValue'}).get('content'))\n",
        "    review_ratings[index] = grade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR status code 521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhtPruvGlU4N",
        "outputId": "7e7c1506-45f3-4187-f893-a45c238f35ed"
      },
      "source": [
        "print(f'overall we have {len(review_ratings)} grades saved')\n",
        "print(f'{len([i for i in review_ratings if review_ratings[i] > 3])} \\\n",
        "of which are 4 or 5')\n",
        "print(f'and {len([i for i in review_ratings if review_ratings[i] < 3])}\\\n",
        " of which are below 3')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overall we have 118 grades saved\n",
            "64 of which are 4 or 5\n",
            "and 38 of which are below 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjEXzvFiT34i"
      },
      "source": [
        "Немного несбалансированно получилось, но мы попробуем сделать на это поправку в будущем. Строго говоря, того что я собрала - достаточно. \n",
        "\n",
        "Формируем обучающую и валидационную выборки. Не руками, конечно. \n",
        "\n",
        "А еще для чистоты эксперимента не рассматриваем отзывы без однозначной оценки: классификация у нас бинарная."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irG42ga2UGPT"
      },
      "source": [
        "from random import sample\n",
        "\n",
        "negative_test = sample([i for i in review_ratings if review_ratings[i] < 3], 5)\n",
        "positive_test = sample([i for i in review_ratings if review_ratings[i] > 3], 5)\n",
        "\n",
        "negative_train = [i for i in review_ratings if review_ratings[i] <= 3 \n",
        "                  and i not in negative_test]\n",
        "postitive_train = [i for i in review_ratings if review_ratings[i] > 3 \n",
        "                   and i not in positive_test]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydL52xQQUpqg"
      },
      "source": [
        "> Токенизируйте слова,  приведите их к нижнему регистру и к начальной форме  (1 балл за токенизацию, 1 - за начальную форму)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAnBaNv5BauD"
      },
      "source": [
        "def create_text_data(indices):\n",
        "    text = ''\n",
        "    for index in indices:\n",
        "        with open(f'review_texts/text_{index}.txt', 'r', encoding='utf-8') as file:\n",
        "            text += '\\n' + file.read()\n",
        "    return text\n",
        "\n",
        "negative_text = create_text_data(negative_train)\n",
        "positive_text = create_text_data(postitive_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMtoqQabm3TT",
        "outputId": "e74f32ca-adef-41b9-b6a1-91aeaf38668b"
      },
      "source": [
        "! pip install spacy==3.0.0 -q\n",
        "! python -m spacy download ru_core_news_sm -q"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.7 MB 214 kB/s \n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 25.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 623 kB 30.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 456 kB 52.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.9 MB 158 kB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 8.9 MB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5QzFHcynJiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf4a759-9a6d-42b3-c7a7-b70638fe59d3"
      },
      "source": [
        "%%time\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('ru_core_news_sm')\n",
        "\n",
        "def preprocess(text):\n",
        "    clean = ''.join([i.lower() for i in text\n",
        "                     if i.isalpha() or i.isspace()]) \n",
        "    tokens = [token.lemma_ for token in nlp(clean) if not token.is_stop \n",
        "          and token.lemma_.isalpha()] \n",
        "    return tokens\n",
        "\n",
        "preprocessed_positive = preprocess(positive_text)\n",
        "preprocessed_negative = preprocess(negative_text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 2s, sys: 2.1 s, total: 1min 4s\n",
            "Wall time: 1min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr6_p9EwUztw"
      },
      "source": [
        "> Составьте 2 множества - в одном будут слова, которые встречаются только в положительных отзывах, а в другом - встречающиеся только в отрицательных. Попробуйте поиграть с частотностями и исключить шум (к примеру, выбросить слова, встречающиеся 1-2 раза) (3 балла) (если у вас получились пустые множества, уберите фильтр по частотности или увеличьте выборку)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v1Qesl7U273"
      },
      "source": [
        "Я надеюсь это не очень критично, что я не обратилась к типу данных *множество*, он не показался мне удобным в данном случае: словарные ключи и так уникальны, но к ним еще и можно намертво прикрепить релевантную информацию о частотности."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5nOnqFYnXi3"
      },
      "source": [
        "def create_frequency_dict(tokens):\n",
        "    freqs = {}\n",
        "    for token in tokens:\n",
        "        if token not in freqs.keys():\n",
        "            freqs[token] = 0\n",
        "        freqs[token] += 1\n",
        "    return freqs\n",
        "\n",
        "freqs_positive = create_frequency_dict(preprocessed_positive)\n",
        "freqs_negative = create_frequency_dict(preprocessed_negative)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OydDCnf9rODM",
        "outputId": "06567107-98c9-4126-aa34-67f4e0da414f"
      },
      "source": [
        "print(f'Negative reviews contain {len(freqs_negative)} unique tokens, whilst positive ones - {len(freqs_positive)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative reviews contain 3639 unique tokens, whilst positive ones - 8396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRbicYsW3JXX",
        "outputId": "826b6166-3310-4af7-88b3-f90e7fd621a9"
      },
      "source": [
        "print('negative most popular tokens')\n",
        "print(*[f'{i[0]} ({i[1]})' for i in sorted(freqs_negative.items(), \n",
        "                                           key=lambda x: x[1], \n",
        "                                           reverse=True)[:10]])\n",
        "print('positive most popular tokens')\n",
        "print(*[f'{i[0]} ({i[1]})' for i in sorted(freqs_positive.items(), \n",
        "                                           key=lambda x: x[1], \n",
        "                                           reverse=True)[:10]])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative most popular tokens\n",
            "волос (505) цвет (214) бальзам (211) тоника (160) оттенок (157) тоник (138) оттеночный (117) очень (102) после (96) результат (69)\n",
            "positive most popular tokens\n",
            "волос (1866) оттенок (792) цвет (769) тоника (728) бальзам (555) после (399) тоник (348) очень (339) раз (222) результат (211)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6UaG93zVH4v"
      },
      "source": [
        "Как и ожидалось, самые абсолютно популярные слова не сильно отличаются для плохих и хороших отзывов: в конце концов эти тексты об одном и том же. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQjH8mFNo3fc"
      },
      "source": [
        "freq_neg_specific = {key: value for key, value in freqs_negative.items() \n",
        "                     if key not in freqs_positive}\n",
        "\n",
        "freq_pos_specific = {key: value for key, value in freqs_positive.items() \n",
        "                     if key not in freqs_negative}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoO0TRwlxvgX",
        "outputId": "5770e76e-2995-44e4-b0e9-47843f6087d8"
      },
      "source": [
        "print(f'Negative reviews contain {len(freq_neg_specific)} unique specific tokens, whilst positive ones - {len(freq_pos_specific)}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative reviews contain 1419 unique specific tokens, whilst positive ones - 6176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgTcHd7Fx_Ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f3f98b-a2d6-4fb5-8cf1-d4e84a95e0ad"
      },
      "source": [
        "print('negative most popular specific tokens')\n",
        "print(*[f'{i[0]} ({i[1]})' for i in sorted(freq_neg_specific.items(), \n",
        "                                           key=lambda x: x[1], \n",
        "                                           reverse=True)[:10]])\n",
        "print('positive most popular specific tokens')\n",
        "print(*[f'{i[0]} ({i[1]})' for i in sorted(freq_pos_specific.items(), \n",
        "                                           key=lambda x: x[1], \n",
        "                                           reverse=True)[:10]])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative most popular specific tokens\n",
            "bloody (5) боль (4) mary (4) огорчить (4) требоваться (4) сомнительный (3) биг (3) пятнистый (3) lux (3) психануть (3)\n",
            "positive most popular specific tokens\n",
            "тонировка (26) мешать (21) менять (19) соотношение (18) го (17) устраивать (16) дальше (16) лично (16) подруга (16) предварительно (15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9btK7wAVSM9"
      },
      "source": [
        "Вот тут уже интереснее получается. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTtre0VyVUhx"
      },
      "source": [
        "Я все-таки хочу убрать слова, встречающиеся единожды, поскольку нередко такие слова - это просто фейлы токенизации, то есть какие-то окказиональные сочетания символов, не то чтобы специфичные для задачи. \n",
        "\n",
        "Также я нормализую встречаемость, превращая ее в вес. Это необходимо сделать из-за дисбаланса классов, ибо условная встречаемость X для фичи негативной тональности будет значить намного больше, чем такая же встречаемость Х для позитивной. \n",
        "\n",
        "Таким образом, каждая специфичная фича будет давать от 0 до 100 баллов отзыву в направлении той или иной тональности. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMkjem2g68Q6",
        "outputId": "a93604ee-9b1e-4582-af73-6776b23c72e7"
      },
      "source": [
        "print(f'number of words that occured just once\\nnegative: \\\n",
        "{len([i for i in freq_neg_specific if freq_neg_specific[i] == 1])}\\\n",
        "\\npositive: {len([i for i in freq_pos_specific if freq_pos_specific[i] == 1])}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of words that occured just once\n",
            "negative: 1325\n",
            "positive: 4861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9xBrh3VzXuI"
      },
      "source": [
        "neg_features = [i for i in freq_neg_specific.items() if not i[1] == 1]\n",
        "pos_features = [i for i in freq_pos_specific.items() if not i[1] == 1]\n",
        "\n",
        "def normalize(features):\n",
        "    x_max = max([i[1] for i in features])\n",
        "    return  [(i[0], int(100 * i[1] / x_max)) for i in features]\n",
        "\n",
        "neg_features_weighted = {i[0]: i[1] \n",
        "                         for i in normalize(neg_features) if i[1] != 0}\n",
        "pos_features_weighted = {i[0]: i[1] \n",
        "                         for i in normalize(pos_features) if i[1] != 0}"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj2EA7rUX8LC"
      },
      "source": [
        ">Создайте функцию, которая будет определять, положительный ли отзыв или отрицательный в зависимости от того, какие слова встретились в нём, и посчитайте качество при помощи accuracy (1  - за коректно работающую функцию, 1 - за подсчёт accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQMmoHEqWIAu"
      },
      "source": [
        "Cформулируем коэффициент поправки на разное количество фичей: позитивных слов у нас в разы больше, чем негативных, а это значит, что вероятность появления какой угодно позитивной фичи намного выше, чем вероятность появления какой угодно негативной.\n",
        "\n",
        "\n",
        "Поэтому мы будем умножать позитивные баллы на отношение количества негативных фичей к позитивным, чтобы привести их примерно к одной шкале."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6WLOVRJK-iI"
      },
      "source": [
        "q = len(neg_features_weighted) / len(pos_features_weighted)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6wxhCJZ6mj4"
      },
      "source": [
        "from random import choice\n",
        "\n",
        "def detect_tonality(text):\n",
        "    tokens = preprocess(text)\n",
        "    positive_points = 0\n",
        "    negative_points = 0\n",
        "    for token in tokens:\n",
        "        if token in pos_features_weighted:\n",
        "            positive_points += pos_features_weighted[token]\n",
        "        elif token in neg_features_weighted:\n",
        "            negative_points += neg_features_weighted[token]\n",
        "    if positive_points * q == negative_points:\n",
        "        return choice(['positive', 'negative'])\n",
        "    return 'positive' if positive_points * q > negative_points else 'negative'\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN3Ji2zCMvHE",
        "outputId": "6ce71b47-d00f-4746-e256-ee4058bcc6d3"
      },
      "source": [
        "test_x = []\n",
        "for collection in [negative_test, positive_test]:\n",
        "    for index in collection:\n",
        "        with open(f'/content/text_{index}.txt', encoding='utf-8') as file:\n",
        "            test_x.append(file.read())\n",
        "\n",
        "test_y = ['negative'] * 5 + ['positive'] * 5\n",
        "\n",
        "def estimate_accuracy(test_x, test_y, model):\n",
        "    if not len(test_x) == len(test_y):\n",
        "        raise ValueError\n",
        "    preds = list(map(model, test_x))\n",
        "    return len([True for i in zip(preds, test_y) if i[0] == i[1]]) / len(test_y)\n",
        "\n",
        "print(estimate_accuracy(test_x, test_y, detect_tonality))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ODGmp6EYUIc"
      },
      "source": [
        "Ну, звучит не так плохо, хотя по факту все не очень: внимательный анализ показал, что выраженный дисбаланс классов привел к тому, что модель с большей охотой относит все подряд к положительным отзывам. И я не могу ее за это осуждать. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCqaq4BuaqdU"
      },
      "source": [
        ">Предложите как минимум 2 способа улучшить эту программу с помощью добавления к ней любых мулек - просто словами, писать улучшающий код не надо (1 балл)\n",
        "\n",
        "\n",
        "1. можно было бы обучать модель не на ~60 положительных и ~30 негативных отзывах, а на ~30 и ~30. Интересно, уравновесит ли выигрыш из-за лучшей сбаланированности потери от меньшего объема. Мне кажется, что вполне может. \n",
        "\n",
        "2. а может можно ввести порог: не добавлять баллы за встреченные фичи в разные счетчики, а просто прибавлять позитивные баллы и отнимать негативные, при этом раздел между тем, что в итоге квалифицируется как негативное, и тем, что мы будем считать позитивным, поставить не 0, а что-нибудь повыше. для определения оптимального порога построить AUC-ROC. эта же метрика покажет нам, насколько вообще наш примитивный подход к классификации хорошо ранжирует. \n",
        "\n",
        "2. можно было бы обратиться к уже [существующим тональным словарям](https://habr.com/ru/post/482052/) и как-то инкорпорировать эту информацию к информации о специфичных токенов для конкретно нашего материала"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X4lO6cWeXXX"
      },
      "source": [
        "> Логичность и чистота кода (1 балл)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "hhy2Jd20IVnM",
        "outputId": "c64b20e3-19b5-4136-e2e2-2e0202f4abd5"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://media.giphy.com/media/jSKikMqhCFo37IezKu/giphy.gif')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://media.giphy.com/media/jSKikMqhCFo37IezKu/giphy.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    }
  ]
}